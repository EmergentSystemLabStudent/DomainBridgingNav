<!DOCTYPE html>
<html>
<head lang="en">
  <meta name="keywords" content="DomainBridgingNav">
  <title>Real-world Instance-specific Image Goal Navigation for Service Robots via Bridging Domain Gap Based on Contrastive Learning</title>
  <style>
    .container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 70px;
      flex-direction: column;
      text-align: center;
    }

    .fixed-width {
      width: 700px;
      margin: 10px auto;
    }

    .center-align {
      text-align: center;
    }

    .text-justify {
      text-align: justify;
    }

    video {
      width: 700px;
      height: auto;
      display: block;
      margin: 10px auto;
    }

    .col-md-8 {
      width: 700px;
      margin: 10px auto;
    }

    .github-link {
      display: flex;
      justify-content: center;
    }

    .github-link a {
      margin: 10px;
    }

    .image-container {
      display: flex;
      justify-content: center;
    }

    .image-container img {
      width: 700px;
      height: auto;
    }

    .video-container {
        margin: 10px;
        display: flex;
        justify-content: center;
    }
    
    .video-container iframe {
        width: 700px;
        height: 394px;
    }
  </style>
</head>
<body>
  <div class="container" id="main">
    <div class="row">
      <h2 class="col-md-12">
         Real-world Instance-specific Image Goal Navigation for Service Robots<br>via Bridging Domain Gap Based on Contrastive Learning
      </h2>
    </div>
  </div>
  
  <p class="center-align">Taichi Sakaguchi, <a href="https://scholar.google.com/citations?hl=en&user=jtB7J0AAAAAJ" target=“_blank” rel=“noopener noreferrer”>Akira Taniguchi</a>, <a href="https://scholar.google.com/citations?hl=en&user=Y4qjYvMAAAAJ" target=“_blank” rel=“noopener noreferrer”>Yoshinobu Hagiwara</a>, <a href="https://scholar.google.co.jp/citations?user=tsm7qaQAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Lotfi El Hafi</a>, <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Shoichi Hasegawa</a>, <a href="https://scholar.google.com/citations?hl=en&user=dPOCLQEAAAAJ" target=“_blank” rel=“noopener noreferrer”>Tadahiro Taniguchi</a></p>
  
  <div class="github-link">
    <a href="https://github.com/EmergentSystemLabStudent/DomainBridgingNav" target=“_blank” rel=“noopener noreferrer”>Github</a>
    <!-- <a href="" target=“_blank” rel=“noopener noreferrer”>Paper</a>
    <a href="" target=“_blank” rel=“noopener noreferrer”>Slide</a> -->
  </div>

  <video src="v2.mp4" style="width: 700px; height: auto;" controls></video>
  
  <!-- <div class="video-container">
      <iframe src="https://www.youtube.com/embed/n8se-MgPi50?si=B5B60-8XXMO-j2CC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div> -->
  
  <h2 class="fixed-width">Abstract</h2>
  <p class="center-align fixed-width text-justify">
    Improving instance-specific image goal navigation (InstanceImageNav), which locates the identical object in a real-world environment from a query image, is essential for robotic systems to assist users in finding desired objects.
    The challenge lies in the domain gap between low-quality images observed by the moving robot, characterized by motion blur and low-resolution, and high-quality query images provided by the user.
    Such domain gaps could significantly reduce the task success rate but have not been the focus of previous work.
    To address this, we propose a novel method called Few-shot Cross-quality Instance-aware Adaptation (CrossIA), which employs contrastive learning with an instance classifier to align features between massive low- and few high-quality images.
    This approach effectively reduces the domain gap by bringing the latent representations of cross-quality images closer on an instance basis.
    Additionally, the system integrates an object image collection with a pre-trained deblurring model to enhance the observed image quality.
    Our method fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. 
    We evaluated our method's effectiveness through an InstanceImageNav task with 20 different types of instances, where the robot identifies the same instance in a real-world environment as a high-quality query image.
    Our experiments showed that our method improves the task success rate by up to three times compared to the baseline, a conventional approach based on SuperGlue.
    These findings highlight the potential of leveraging contrastive learning and image enhancement techniques to bridge the domain gap and improve object localization in robotic applications.  </p>

  <h2 class="fixed-width">Approach</h2>
  <p class="center-align fixed-width text-justify">
    As shown in Figure, the proposed system is divided into three main modules.
    Firstly, the Data Collection Module constructs the 3D semantic map of the environment and then collects object images.
    Secondly, the Fine-tuning Module fine-tunes the pre-trained models using the collected object images and few-shot high-quality images provided by the user through contrastive learning.
    Our contrastive learning is called <b> Few-shot Cross-quality Instance-aware Adaptation (CrossIA) <\b>.
    Lastly, the Navigation Module leverages the fine-tuned model and the semantic map to locate objects identical to the query image.  </p>
  <div class="image-container">
    <img src="./images/real_approach.svg">
  </div>

  <h2 class="fixed-width">Data Collection Module</h2>
  <p class="center-align fixed-width text-justify">
    First, the robot acquires multiple images of its surroundings with the head camera. Then, the images are input to ClipCap for caption generation, and the images are input to CNN to obtain image features. The robot learns location concepts by categorizing these, plus self-position, into three multimodal pieces of information.
  </p>

  <h2 class="fixed-width">Fine-tuning Module</h2>
  <p class="center-align fixed-width text-justify">
    First, the robot acquires multiple images of its surroundings with the head camera. Then, the images are input to ClipCap for caption generation, and the images are input to CNN to obtain image features. The robot learns location concepts by categorizing these, plus self-position, into three multimodal pieces of information.
  </p>

  <h2 class="fixed-width">Navigation Module</h2>
  <p class="center-align fixed-width text-justify">
    First, the robot acquires multiple images of its surroundings with the head camera. Then, the images are input to ClipCap for caption generation, and the images are input to CNN to obtain image features. The robot learns location concepts by categorizing these, plus self-position, into three multimodal pieces of information.
  </p>

  <div class="col-md-8">
    <h2 class="text-center">Citation</h2>
    <p class="text-justify">
      This paper is under review on IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024), and the BibTeX for the paper is below.
      <textarea id="bibtex" class="form-control" readonly rows="6" cols="105">
@inproceedings{domainbridgingnav2024,
  title={Real-world Instance-specific Image Goal Navigation for Service Robots via Bridging Domain Gap Based on Contrastive Learning},
  author={Taichi Sakaguchi and Akira Taniguchi and Yoshinobu Hagiwara and Lotfi El Hafi and Shoichi Hasegawa and Tadahiro Taniguchi},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2024 UnderReview}
}</textarea>
    </p>
  </div>

  <div class="col-md-8">
    <h2 class="text-center">Other links</h2>
    <p class="text-justify">
      <ul>
        <li><a href="http://www.em.ci.ritsumei.ac.jp/" target=“_blank” rel=“noopener noreferrer”>Laboratory website</a></li>
        <li><a href="" target=“_blank” rel=“noopener noreferrer”>Demo video of this research</a></li>
        <li><a href="https://www.youtube.com/watch?v=UBgZGRG00eA" target=“_blank” rel=“noopener noreferrer”>Demo video of related research</a></li>
      </ul>
    </p>
  </div>

  <div class="col-md-8">
    <h2 class="text-center">Acknowledgements</h2>
    <p class="text-justify">
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212) JST Moonshot Research & Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
  
  <script src="script.js"></script>
</body>
</html>
